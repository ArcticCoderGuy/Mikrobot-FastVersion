"""
Performance Monitor
Comprehensive monitoring and quality metrics collection system
Tracks M5 BOS + M1 retest validation performance, execution quality, and system health
"""

from typing import Dict, Any, Optional, List, Tuple
from datetime import datetime, timedelta
import logging
import statistics
import json
from dataclasses import dataclass, asdict
from enum import Enum
from collections import defaultdict, deque
import time

logger = logging.getLogger(__name__)


class PerformanceLevel(Enum):
    """Performance level classifications"""
    EXCELLENT = "excellent"
    GOOD = "good"
    ACCEPTABLE = "acceptable"
    POOR = "poor"
    CRITICAL = "critical"


class AlertSeverity(Enum):
    """Alert severity levels"""
    INFO = "info"
    WARNING = "warning"
    ERROR = "error"
    CRITICAL = "critical"


@dataclass
class PerformanceAlert:
    """Performance alert"""
    timestamp: datetime
    severity: AlertSeverity
    component: str
    metric: str
    current_value: float
    threshold_value: float
    message: str
    trace_id: Optional[str] = None


@dataclass
class ValidationPerformanceMetrics:
    """Validation performance metrics"""
    total_validations: int = 0
    successful_validations: int = 0
    failed_validations: int = 0
    avg_validation_time_ms: float = 0.0
    min_validation_time_ms: float = 0.0
    max_validation_time_ms: float = 0.0
    p95_validation_time_ms: float = 0.0
    p99_validation_time_ms: float = 0.0
    success_rate: float = 0.0
    sub_100ms_rate: float = 0.0
    cache_hit_rate: float = 0.0
    error_rate: float = 0.0


@dataclass
class PatternPerformanceMetrics:
    """Pattern-specific performance metrics"""
    pattern_type: str
    total_patterns: int = 0
    approved_patterns: int = 0
    rejected_patterns: int = 0
    avg_confidence: float = 0.0
    approval_rate: float = 0.0
    avg_processing_time_ms: float = 0.0
    quality_scores: List[float] = None
    
    def __post_init__(self):
        if self.quality_scores is None:
            self.quality_scores = []


@dataclass
class SystemHealthMetrics:
    """System health metrics"""
    cpu_usage_percent: float = 0.0
    memory_usage_percent: float = 0.0
    disk_usage_percent: float = 0.0
    network_latency_ms: float = 0.0
    error_count_last_hour: int = 0
    uptime_seconds: int = 0
    active_connections: int = 0
    queue_size: int = 0
    health_score: float = 1.0
    status: str = "healthy"


class PerformanceMonitor:
    """
    Comprehensive performance monitoring system
    
    Features:
    - Real-time validation performance tracking
    - Pattern-specific quality metrics
    - System health monitoring
    - Alert generation and management
    - Performance trend analysis
    - Quality assurance reporting
    """
    
    def __init__(self):
        self.start_time = datetime.utcnow()
        
        # Performance thresholds
        self.performance_thresholds = {
            'validation_time_ms': {
                'excellent': 50.0,
                'good': 75.0,
                'acceptable': 100.0,
                'poor': 150.0,
                'critical': 200.0
            },
            'success_rate': {
                'excellent': 0.98,
                'good': 0.95,
                'acceptable': 0.90,
                'poor': 0.85,
                'critical': 0.80
            },
            'confidence_score': {
                'excellent': 0.90,
                'good': 0.85,
                'acceptable': 0.80,
                'poor': 0.75,
                'critical': 0.70
            },
            'approval_rate': {
                'excellent': 0.70,
                'good': 0.60,
                'acceptable': 0.50,
                'poor': 0.40,
                'critical': 0.30
            }
        }\n        \n        # Metrics storage\n        self.validation_metrics = ValidationPerformanceMetrics()\n        self.pattern_metrics = {\n            'M5_BOS': PatternPerformanceMetrics('M5_BOS'),\n            'M1_BREAK_RETEST': PatternPerformanceMetrics('M1_BREAK_RETEST')\n        }\n        self.system_health = SystemHealthMetrics()\n        \n        # Performance history (sliding windows)\n        self.validation_times = deque(maxlen=1000)  # Last 1000 validations\n        self.validation_results = deque(maxlen=1000)  # Last 1000 results\n        self.confidence_scores = deque(maxlen=1000)  # Last 1000 confidence scores\n        self.hourly_metrics = deque(maxlen=168)  # Last 7 days of hourly metrics\n        \n        # Alert management\n        self.active_alerts: List[PerformanceAlert] = []\n        self.alert_history: List[PerformanceAlert] = []\n        self.alert_thresholds = {\n            'validation_time_ms': 150.0,\n            'success_rate': 0.85,\n            'error_rate': 0.10,\n            'memory_usage_percent': 85.0,\n            'cpu_usage_percent': 80.0\n        }\n        \n        # Quality assurance tracking\n        self.qa_metrics = {\n            'false_positive_rate': 0.0,\n            'false_negative_rate': 0.0,\n            'pattern_accuracy_m5_bos': 0.0,\n            'pattern_accuracy_m1_retest': 0.0,\n            'overall_quality_score': 0.0,\n            'quality_trend': 'stable'\n        }\n        \n        # Performance counters\n        self.counters = defaultdict(int)\n        self.timers = defaultdict(list)\n        \n        logger.info(\"Performance Monitor initialized\")\n    \n    def record_validation_performance(self, \n                                    validation_time_ms: float,\n                                    success: bool,\n                                    confidence: float,\n                                    pattern_type: str,\n                                    cache_hit: bool = False,\n                                    trace_id: str = None):\n        \"\"\"Record validation performance metrics\"\"\"\n        \n        try:\n            # Update overall validation metrics\n            self.validation_metrics.total_validations += 1\n            \n            if success:\n                self.validation_metrics.successful_validations += 1\n            else:\n                self.validation_metrics.failed_validations += 1\n            \n            # Record timing\n            self.validation_times.append(validation_time_ms)\n            self.validation_results.append(success)\n            self.confidence_scores.append(confidence)\n            \n            # Update timing statistics\n            if len(self.validation_times) > 0:\n                times = list(self.validation_times)\n                self.validation_metrics.avg_validation_time_ms = statistics.mean(times)\n                self.validation_metrics.min_validation_time_ms = min(times)\n                self.validation_metrics.max_validation_time_ms = max(times)\n                \n                if len(times) >= 20:  # Need minimum samples for percentiles\n                    sorted_times = sorted(times)\n                    self.validation_metrics.p95_validation_time_ms = sorted_times[int(0.95 * len(sorted_times))]\n                    self.validation_metrics.p99_validation_time_ms = sorted_times[int(0.99 * len(sorted_times))]\n            \n            # Update rates\n            total = self.validation_metrics.total_validations\n            self.validation_metrics.success_rate = self.validation_metrics.successful_validations / total\n            self.validation_metrics.error_rate = self.validation_metrics.failed_validations / total\n            \n            # Sub-100ms performance tracking\n            sub_100ms_count = sum(1 for t in self.validation_times if t < 100.0)\n            self.validation_metrics.sub_100ms_rate = sub_100ms_count / len(self.validation_times)\n            \n            # Cache performance (if applicable)\n            if cache_hit:\n                self.counters['cache_hits'] += 1\n            else:\n                self.counters['cache_misses'] += 1\n            \n            total_requests = self.counters['cache_hits'] + self.counters['cache_misses']\n            if total_requests > 0:\n                self.validation_metrics.cache_hit_rate = self.counters['cache_hits'] / total_requests\n            \n            # Pattern-specific metrics\n            if pattern_type in self.pattern_metrics:\n                pattern = self.pattern_metrics[pattern_type]\n                pattern.total_patterns += 1\n                \n                if success:\n                    pattern.approved_patterns += 1\n                else:\n                    pattern.rejected_patterns += 1\n                \n                # Update pattern averages\n                pattern.approval_rate = pattern.approved_patterns / pattern.total_patterns\n                \n                # Update confidence average\n                current_avg = pattern.avg_confidence\n                pattern.avg_confidence = ((current_avg * (pattern.total_patterns - 1)) + confidence) / pattern.total_patterns\n                \n                # Update processing time average\n                current_time_avg = pattern.avg_processing_time_ms\n                pattern.avg_processing_time_ms = ((current_time_avg * (pattern.total_patterns - 1)) + validation_time_ms) / pattern.total_patterns\n                \n                # Store quality score\n                pattern.quality_scores.append(confidence)\n                if len(pattern.quality_scores) > 100:  # Keep last 100\n                    pattern.quality_scores = pattern.quality_scores[-100:]\n            \n            # Check for performance alerts\n            self._check_performance_alerts(validation_time_ms, success, confidence, trace_id)\n            \n        except Exception as e:\n            logger.error(f\"Error recording validation performance: {str(e)}\")\n    \n    def record_system_health(self, \n                           cpu_usage: float,\n                           memory_usage: float,\n                           disk_usage: float = 0.0,\n                           network_latency: float = 0.0,\n                           active_connections: int = 0,\n                           queue_size: int = 0):\n        \"\"\"Record system health metrics\"\"\"\n        \n        try:\n            self.system_health.cpu_usage_percent = cpu_usage\n            self.system_health.memory_usage_percent = memory_usage\n            self.system_health.disk_usage_percent = disk_usage\n            self.system_health.network_latency_ms = network_latency\n            self.system_health.active_connections = active_connections\n            self.system_health.queue_size = queue_size\n            \n            # Calculate uptime\n            self.system_health.uptime_seconds = int((datetime.utcnow() - self.start_time).total_seconds())\n            \n            # Calculate health score\n            health_factors = []\n            \n            # CPU health (lower is better)\n            if cpu_usage < 50:\n                health_factors.append(1.0)\n            elif cpu_usage < 70:\n                health_factors.append(0.8)\n            elif cpu_usage < 85:\n                health_factors.append(0.6)\n            else:\n                health_factors.append(0.3)\n            \n            # Memory health (lower is better)\n            if memory_usage < 60:\n                health_factors.append(1.0)\n            elif memory_usage < 80:\n                health_factors.append(0.8)\n            elif memory_usage < 90:\n                health_factors.append(0.5)\n            else:\n                health_factors.append(0.2)\n            \n            # Network health (lower latency is better)\n            if network_latency < 50:\n                health_factors.append(1.0)\n            elif network_latency < 100:\n                health_factors.append(0.8)\n            elif network_latency < 200:\n                health_factors.append(0.6)\n            else:\n                health_factors.append(0.4)\n            \n            # Overall health score\n            self.system_health.health_score = statistics.mean(health_factors)\n            \n            # System status\n            if self.system_health.health_score >= 0.9:\n                self.system_health.status = \"excellent\"\n            elif self.system_health.health_score >= 0.8:\n                self.system_health.status = \"good\"\n            elif self.system_health.health_score >= 0.6:\n                self.system_health.status = \"acceptable\"\n            elif self.system_health.health_score >= 0.4:\n                self.system_health.status = \"poor\"\n            else:\n                self.system_health.status = \"critical\"\n            \n            # Check system health alerts\n            self._check_system_health_alerts()\n            \n        except Exception as e:\n            logger.error(f\"Error recording system health: {str(e)}\")\n    \n    def record_trade_outcome(self, \n                           pattern_type: str,\n                           predicted_outcome: str,\n                           actual_outcome: str,\n                           confidence: float,\n                           pnl: float,\n                           trace_id: str):\n        \"\"\"Record trade outcome for quality assurance\"\"\"\n        \n        try:\n            # Track prediction accuracy\n            prediction_correct = (predicted_outcome == actual_outcome)\n            \n            # Update quality metrics based on pattern type\n            if pattern_type == 'M5_BOS':\n                self._update_pattern_accuracy('pattern_accuracy_m5_bos', prediction_correct)\n            elif pattern_type == 'M1_BREAK_RETEST':\n                self._update_pattern_accuracy('pattern_accuracy_m1_retest', prediction_correct)\n            \n            # Track false positives/negatives\n            if predicted_outcome == 'positive' and actual_outcome == 'negative':\n                self.counters['false_positives'] += 1\n            elif predicted_outcome == 'negative' and actual_outcome == 'positive':\n                self.counters['false_negatives'] += 1\n            elif predicted_outcome == 'positive' and actual_outcome == 'positive':\n                self.counters['true_positives'] += 1\n            else:\n                self.counters['true_negatives'] += 1\n            \n            # Update false positive/negative rates\n            total_predictions = (self.counters['false_positives'] + \n                               self.counters['false_negatives'] + \n                               self.counters['true_positives'] + \n                               self.counters['true_negatives'])\n            \n            if total_predictions > 0:\n                self.qa_metrics['false_positive_rate'] = self.counters['false_positives'] / total_predictions\n                self.qa_metrics['false_negative_rate'] = self.counters['false_negatives'] / total_predictions\n            \n            # Update overall quality score\n            accuracy_scores = [\n                self.qa_metrics['pattern_accuracy_m5_bos'],\n                self.qa_metrics['pattern_accuracy_m1_retest']\n            ]\n            \n            valid_scores = [score for score in accuracy_scores if score > 0]\n            if valid_scores:\n                self.qa_metrics['overall_quality_score'] = statistics.mean(valid_scores)\n            \n            logger.debug(f\"Trade outcome recorded: {pattern_type} - {predicted_outcome} vs {actual_outcome} (PnL: {pnl})\")\n            \n        except Exception as e:\n            logger.error(f\"Error recording trade outcome: {str(e)}\")\n    \n    def _update_pattern_accuracy(self, pattern_key: str, correct: bool):\n        \"\"\"Update pattern accuracy metric\"\"\"\n        current_accuracy = self.qa_metrics[pattern_key]\n        current_count = self.counters[f\"{pattern_key}_count\"]\n        \n        self.counters[f\"{pattern_key}_count\"] += 1\n        new_count = self.counters[f\"{pattern_key}_count\"]\n        \n        # Update running average\n        if current_count == 0:\n            self.qa_metrics[pattern_key] = 1.0 if correct else 0.0\n        else:\n            self.qa_metrics[pattern_key] = ((current_accuracy * current_count) + (1.0 if correct else 0.0)) / new_count\n    \n    def _check_performance_alerts(self, validation_time_ms: float, success: bool, confidence: float, trace_id: str):\n        \"\"\"Check for performance-related alerts\"\"\"\n        \n        # Validation time alert\n        if validation_time_ms > self.alert_thresholds['validation_time_ms']:\n            alert = PerformanceAlert(\n                timestamp=datetime.utcnow(),\n                severity=AlertSeverity.WARNING,\n                component=\"validation\",\n                metric=\"validation_time_ms\",\n                current_value=validation_time_ms,\n                threshold_value=self.alert_thresholds['validation_time_ms'],\n                message=f\"Validation time {validation_time_ms:.1f}ms exceeds threshold\",\n                trace_id=trace_id\n            )\n            self._add_alert(alert)\n        \n        # Success rate alert\n        if self.validation_metrics.success_rate < self.alert_thresholds['success_rate']:\n            alert = PerformanceAlert(\n                timestamp=datetime.utcnow(),\n                severity=AlertSeverity.ERROR,\n                component=\"validation\",\n                metric=\"success_rate\",\n                current_value=self.validation_metrics.success_rate,\n                threshold_value=self.alert_thresholds['success_rate'],\n                message=f\"Success rate {self.validation_metrics.success_rate:.1%} below threshold\",\n                trace_id=trace_id\n            )\n            self._add_alert(alert)\n        \n        # Error rate alert\n        if self.validation_metrics.error_rate > self.alert_thresholds['error_rate']:\n            alert = PerformanceAlert(\n                timestamp=datetime.utcnow(),\n                severity=AlertSeverity.ERROR,\n                component=\"validation\",\n                metric=\"error_rate\",\n                current_value=self.validation_metrics.error_rate,\n                threshold_value=self.alert_thresholds['error_rate'],\n                message=f\"Error rate {self.validation_metrics.error_rate:.1%} exceeds threshold\",\n                trace_id=trace_id\n            )\n            self._add_alert(alert)\n    \n    def _check_system_health_alerts(self):\n        \"\"\"Check for system health alerts\"\"\"\n        \n        # CPU usage alert\n        if self.system_health.cpu_usage_percent > self.alert_thresholds['cpu_usage_percent']:\n            alert = PerformanceAlert(\n                timestamp=datetime.utcnow(),\n                severity=AlertSeverity.WARNING,\n                component=\"system\",\n                metric=\"cpu_usage_percent\",\n                current_value=self.system_health.cpu_usage_percent,\n                threshold_value=self.alert_thresholds['cpu_usage_percent'],\n                message=f\"CPU usage {self.system_health.cpu_usage_percent:.1f}% exceeds threshold\"\n            )\n            self._add_alert(alert)\n        \n        # Memory usage alert\n        if self.system_health.memory_usage_percent > self.alert_thresholds['memory_usage_percent']:\n            alert = PerformanceAlert(\n                timestamp=datetime.utcnow(),\n                severity=AlertSeverity.WARNING,\n                component=\"system\",\n                metric=\"memory_usage_percent\",\n                current_value=self.system_health.memory_usage_percent,\n                threshold_value=self.alert_thresholds['memory_usage_percent'],\n                message=f\"Memory usage {self.system_health.memory_usage_percent:.1f}% exceeds threshold\"\n            )\n            self._add_alert(alert)\n    \n    def _add_alert(self, alert: PerformanceAlert):\n        \"\"\"Add performance alert\"\"\"\n        # Check if similar alert already exists (avoid spam)\n        existing_alert = next(\n            (a for a in self.active_alerts \n             if a.component == alert.component and a.metric == alert.metric),\n            None\n        )\n        \n        if not existing_alert:\n            self.active_alerts.append(alert)\n            self.alert_history.append(alert)\n            \n            # Keep alert history limited\n            if len(self.alert_history) > 1000:\n                self.alert_history = self.alert_history[-1000:]\n            \n            logger.warning(f\"Performance alert: {alert.message}\")\n    \n    def clear_alert(self, component: str, metric: str):\n        \"\"\"Clear specific alert\"\"\"\n        self.active_alerts = [\n            alert for alert in self.active_alerts \n            if not (alert.component == component and alert.metric == metric)\n        ]\n    \n    def get_performance_level(self, metric_name: str, value: float) -> PerformanceLevel:\n        \"\"\"Get performance level for a metric value\"\"\"\n        if metric_name not in self.performance_thresholds:\n            return PerformanceLevel.ACCEPTABLE\n        \n        thresholds = self.performance_thresholds[metric_name]\n        \n        if value >= thresholds['excellent']:\n            return PerformanceLevel.EXCELLENT\n        elif value >= thresholds['good']:\n            return PerformanceLevel.GOOD\n        elif value >= thresholds['acceptable']:\n            return PerformanceLevel.ACCEPTABLE\n        elif value >= thresholds['poor']:\n            return PerformanceLevel.POOR\n        else:\n            return PerformanceLevel.CRITICAL\n    \n    def get_comprehensive_metrics(self) -> Dict[str, Any]:\n        \"\"\"Get comprehensive performance metrics\"\"\"\n        return {\n            'validation_performance': asdict(self.validation_metrics),\n            'pattern_performance': {\n                pattern_type: asdict(metrics) \n                for pattern_type, metrics in self.pattern_metrics.items()\n            },\n            'system_health': asdict(self.system_health),\n            'quality_assurance': self.qa_metrics,\n            'active_alerts': [asdict(alert) for alert in self.active_alerts],\n            'performance_levels': {\n                'validation_time': self.get_performance_level(\n                    'validation_time_ms', \n                    self.validation_metrics.avg_validation_time_ms\n                ).value,\n                'success_rate': self.get_performance_level(\n                    'success_rate', \n                    self.validation_metrics.success_rate\n                ).value,\n                'overall_confidence': self.get_performance_level(\n                    'confidence_score',\n                    statistics.mean(self.confidence_scores) if self.confidence_scores else 0.0\n                ).value\n            },\n            'counters': dict(self.counters),\n            'uptime_hours': round(self.system_health.uptime_seconds / 3600, 2)\n        }\n    \n    def get_quality_report(self) -> Dict[str, Any]:\n        \"\"\"Generate comprehensive quality report\"\"\"\n        metrics = self.get_comprehensive_metrics()\n        \n        # Overall quality grade\n        quality_score = self.qa_metrics['overall_quality_score']\n        if quality_score >= 0.95:\n            quality_grade = 'A+'\n        elif quality_score >= 0.90:\n            quality_grade = 'A'\n        elif quality_score >= 0.85:\n            quality_grade = 'B+'\n        elif quality_score >= 0.80:\n            quality_grade = 'B'\n        elif quality_score >= 0.75:\n            quality_grade = 'C'\n        else:\n            quality_grade = 'D'\n        \n        # Performance summary\n        performance_summary = {\n            'validation_performance_grade': self.get_performance_level(\n                'validation_time_ms', \n                self.validation_metrics.avg_validation_time_ms\n            ).value,\n            'success_rate_grade': self.get_performance_level(\n                'success_rate', \n                self.validation_metrics.success_rate\n            ).value,\n            'confidence_grade': self.get_performance_level(\n                'confidence_score',\n                statistics.mean(self.confidence_scores) if self.confidence_scores else 0.0\n            ).value\n        }\n        \n        # Recommendations\n        recommendations = []\n        \n        if self.validation_metrics.avg_validation_time_ms > 100:\n            recommendations.append(f\"Average validation time {self.validation_metrics.avg_validation_time_ms:.1f}ms exceeds 100ms target\")\n        \n        if self.validation_metrics.success_rate < 0.9:\n            recommendations.append(f\"Success rate {self.validation_metrics.success_rate:.1%} below 90% target\")\n        \n        if self.qa_metrics['false_positive_rate'] > 0.1:\n            recommendations.append(f\"False positive rate {self.qa_metrics['false_positive_rate']:.1%} exceeds 10% threshold\")\n        \n        if len(self.active_alerts) > 0:\n            recommendations.append(f\"{len(self.active_alerts)} active performance alerts require attention\")\n        \n        if self.system_health.health_score < 0.8:\n            recommendations.append(f\"System health score {self.system_health.health_score:.2f} indicates performance issues\")\n        \n        return {\n            'overall_quality_grade': quality_grade,\n            'quality_score': round(quality_score, 3),\n            'performance_summary': performance_summary,\n            'recommendations': recommendations,\n            'detailed_metrics': metrics,\n            'alert_summary': {\n                'active_alerts': len(self.active_alerts),\n                'critical_alerts': len([a for a in self.active_alerts if a.severity == AlertSeverity.CRITICAL]),\n                'warning_alerts': len([a for a in self.active_alerts if a.severity == AlertSeverity.WARNING])\n            },\n            'trend_analysis': self._analyze_performance_trends()\n        }\n    \n    def _analyze_performance_trends(self) -> Dict[str, str]:\n        \"\"\"Analyze performance trends\"\"\"\n        trends = {}\n        \n        # Validation time trend\n        if len(self.validation_times) >= 50:\n            recent_times = list(self.validation_times)[-25:]\n            older_times = list(self.validation_times)[-50:-25]\n            \n            recent_avg = statistics.mean(recent_times)\n            older_avg = statistics.mean(older_times)\n            \n            if recent_avg < older_avg * 0.9:\n                trends['validation_time'] = 'improving'\n            elif recent_avg > older_avg * 1.1:\n                trends['validation_time'] = 'degrading'\n            else:\n                trends['validation_time'] = 'stable'\n        else:\n            trends['validation_time'] = 'insufficient_data'\n        \n        # Success rate trend\n        if len(self.validation_results) >= 50:\n            recent_results = list(self.validation_results)[-25:]\n            older_results = list(self.validation_results)[-50:-25]\n            \n            recent_success_rate = sum(recent_results) / len(recent_results)\n            older_success_rate = sum(older_results) / len(older_results)\n            \n            if recent_success_rate > older_success_rate + 0.05:\n                trends['success_rate'] = 'improving'\n            elif recent_success_rate < older_success_rate - 0.05:\n                trends['success_rate'] = 'degrading'\n            else:\n                trends['success_rate'] = 'stable'\n        else:\n            trends['success_rate'] = 'insufficient_data'\n        \n        # Confidence trend\n        if len(self.confidence_scores) >= 50:\n            recent_confidence = list(self.confidence_scores)[-25:]\n            older_confidence = list(self.confidence_scores)[-50:-25]\n            \n            recent_avg = statistics.mean(recent_confidence)\n            older_avg = statistics.mean(older_confidence)\n            \n            if recent_avg > older_avg + 0.05:\n                trends['confidence'] = 'improving'\n            elif recent_avg < older_avg - 0.05:\n                trends['confidence'] = 'degrading'\n            else:\n                trends['confidence'] = 'stable'\n        else:\n            trends['confidence'] = 'insufficient_data'\n        \n        return trends\n    \n    def export_metrics(self, format_type: str = \"json\") -> str:\n        \"\"\"Export metrics in specified format\"\"\"\n        metrics = self.get_comprehensive_metrics()\n        \n        if format_type.lower() == \"json\":\n            return json.dumps(metrics, indent=2, default=str)\n        else:\n            raise ValueError(f\"Unsupported export format: {format_type}\")\n    \n    def reset_metrics(self):\n        \"\"\"Reset all metrics (for testing or fresh start)\"\"\"\n        self.validation_metrics = ValidationPerformanceMetrics()\n        self.pattern_metrics = {\n            'M5_BOS': PatternPerformanceMetrics('M5_BOS'),\n            'M1_BREAK_RETEST': PatternPerformanceMetrics('M1_BREAK_RETEST')\n        }\n        self.system_health = SystemHealthMetrics()\n        \n        self.validation_times.clear()\n        self.validation_results.clear()\n        self.confidence_scores.clear()\n        \n        self.active_alerts.clear()\n        self.alert_history.clear()\n        \n        self.counters.clear()\n        self.timers.clear()\n        \n        self.start_time = datetime.utcnow()\n        \n        logger.info(\"Performance metrics reset\")"